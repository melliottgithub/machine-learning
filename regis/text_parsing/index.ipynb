{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text_Parsing test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"This is a sample text. It contains some words, and stop words like 'the' and 'is'.\"\n",
    "original_text = test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comparison():\n",
    "    print(\"Original text: \", original_text)\n",
    "    print(\"Test text:     \", test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  This is a sample text. It contains some words, and stop words like 'the' and 'is'.\n",
      "Test text:      this is a sample text. it contains some words, and stop words like 'the' and 'is'.\n"
     ]
    }
   ],
   "source": [
    "test_text = test_text.lower()\n",
    "print_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuation and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  This is a sample text. It contains some words, and stop words like 'the' and 'is'.\n",
      "Test text:      this is a sample text  it contains some words  and stop words like  the  and  is  \n"
     ]
    }
   ],
   "source": [
    "# should I remove numbers too or just letters? [^a-zA-Z0-9] --- Will probably depend on each use case\n",
    "test_text = re.sub(r\"[^a-zA-Z]\", \" \", test_text)\n",
    "print_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kllmm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = stopwords.words(\"english\")\n",
    "sw[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'sample', 'text', 'it', 'contains', 'some', 'words', 'and']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = test_text.split()\n",
    "test_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample', 'text', 'contains', 'words', 'stop', 'words', 'like', 'and', 'is']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in test_text:\n",
    "    if word in sw:\n",
    "        test_text.remove(word)\n",
    "test_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization or Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using stemming for speed\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sampl', 'text', 'contain', 'word', 'stop', 'word', 'like', 'and', 'is']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(test_text)):\n",
    "    test_text[i] = ps.stem(test_text[i])\n",
    "test_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sampl text contain word stop word like and is'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = \" \".join(test_text)\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  This is a sample text. It contains some words, and stop words like 'the' and 'is'.\n",
      "Test text:      sampl text contain word stop word like and is\n"
     ]
    }
   ],
   "source": [
    "print_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sampl': 1,\n",
       " 'text': 1,\n",
       " 'contain': 1,\n",
       " 'word': 2,\n",
       " 'stop': 1,\n",
       " 'like': 1,\n",
       " 'and': 1,\n",
       " 'is': 1}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = {}\n",
    "\n",
    "for word in test_text.split():\n",
    "    if word in dictionary:\n",
    "        dictionary[word] += 1\n",
    "    else:\n",
    "        dictionary[word] = 1\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampl text contain word stop word like\n"
     ]
    }
   ],
   "source": [
    "def text_parse(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z]',' ', text)\n",
    "    text = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [ps.stem(word) for word in text if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "text = text_parse(original_text)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sampl': 1, 'text': 1, 'contain': 1, 'word': 2, 'stop': 1, 'like': 1}\n",
      "Counter({'word': 2, 'sampl': 1, 'text': 1, 'contain': 1, 'stop': 1, 'like': 1})\n",
      "[('word', 2), ('sampl', 1), ('text', 1), ('contain', 1), ('stop', 1), ('like', 1)]\n",
      "word: 2\n",
      "sampl: 1\n",
      "text: 1\n",
      "contain: 1\n",
      "stop: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens = text.split()\n",
    "most_ocurrence = {word: tokens.count(word) for word in tokens}\n",
    "print(most_ocurrence)\n",
    "print(Counter(tokens))\n",
    "\n",
    "sorted_count = sorted(most_ocurrence.items(), \\\n",
    "    key=lambda val: val[1], reverse=True)\n",
    "print(sorted_count)\n",
    "\n",
    "for word, count in sorted_count[:5]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq_analysis(text, top_n=5):\n",
    "    tokens= text.split()\n",
    "    words_count= Counter(tokens)\n",
    "\n",
    "    sorted_count = sorted(words_count.items(), \\\n",
    "         key=lambda val:val[1], reverse=True)\n",
    "\n",
    "    for word, count in sorted_count[:top_n]:\n",
    "        print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = './gutenberg.org_cache_epub_71894_pg71894.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hellen: 264\n",
      "cyru: 205\n",
      "king: 176\n",
      "great: 175\n",
      "would: 171\n",
      "could: 155\n",
      "one: 154\n",
      "day: 133\n",
      "soldier: 129\n",
      "time: 124\n"
     ]
    }
   ],
   "source": [
    "with open(text_file, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "gutt_book = text_parse(text)\n",
    "word_freq_analysis(gutt_book,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "At first, case normalization was performed, making the text easier to analyze. This step also ensures that the same word, appearing in different cases, is treated as a single entity.\n",
    "\n",
    "Next, punctuation and numbers were removed from the text. This is crucial for eliminating 'noise' that doesn't contribute to the meaning of the text, making it more straightforward to process and analyze.\n",
    "\n",
    "Following this, stopwords were filtered out. These are common words that don't carry significant meaning in text analysis. \n",
    "\n",
    "After that, techniques like lemmatization or stemming were applied. The goal here is to reduce words to their root or base form. This simplifies the vocabulary while retaining the core meaning of each word, which is beneficial for various NLP tasks that we will be working in the future.\n",
    "\n",
    "Lastly, a counter test was implemented to count the frequency of words or elements in the text.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Text parsing is an indispensable piece in the field of text analytics, and the comprehensive techniques used in this notebook are highly beneficial for a wide array of future applications. \n",
    "\n",
    "The preprocessing steps I've carefully outlined serve not just as a set of techniques but as a robust foundation that can be seamlessly integrated and adapted across several NLP projects. \n",
    "\n",
    "Whether the focus shifts towards sentiment analysis to gauge customer opinions, text classification for automating document sorting, the development of intelligent conversational chatbots for customer service, or the fine-tuning of search engine algorithms for more accurate query results, this notebook will act as a foundational guide. \n",
    "\n",
    "The foundation established here is designed to significantly elevate the performance, accuracy, and reliability of any NLP projects or machine learning project we will delve in the future.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future improvements below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 'gutenberg'),\n",
       " ('gutenberg', 'ebook'),\n",
       " ('ebook', 'retreat'),\n",
       " ('retreat', 'ten'),\n",
       " ('ten', 'thousand'),\n",
       " ('thousand', 'ebook'),\n",
       " ('ebook', 'use'),\n",
       " ('use', 'anyon'),\n",
       " ('anyon', 'anywher'),\n",
       " ('anywher', 'unit')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(gutt_book.split(), 2))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 'gutenberg', 'ebook'),\n",
       " ('gutenberg', 'ebook', 'retreat'),\n",
       " ('ebook', 'retreat', 'ten'),\n",
       " ('retreat', 'ten', 'thousand'),\n",
       " ('ten', 'thousand', 'ebook'),\n",
       " ('thousand', 'ebook', 'use'),\n",
       " ('ebook', 'use', 'anyon'),\n",
       " ('use', 'anyon', 'anywher'),\n",
       " ('anyon', 'anywher', 'unit'),\n",
       " ('anywher', 'unit', 'state')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(gutt_book.split(), 3))[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
